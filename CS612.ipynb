{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15hoHxkyxACmlw-rIBOa1ZoJ5Wvc531HQ",
      "authorship_tag": "ABX9TyMkRgYDPzw8np5JD9jF1eX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haridasaravind/CS612/blob/main/CS612.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K8ivuQU2crm",
        "outputId": "7863d57b-2b9f-43fa-c0f5-d97e3bbd2b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsHokfEO2mf6",
        "outputId": "5507d6ce-8333-4f28-dc7f-2b36f3bf4f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "def read_fasta(directory='.'):\n",
        "    # Obtain a list of all files with the .fas extension in the specified directory\n",
        "    fas_files = [f for f in os.listdir(directory) if f.endswith('.fas')]\n",
        "    # Select a random .fas file from the list\n",
        "    selected_file = random.choice(fas_files)\n",
        "    print(f\"Selected File: {selected_file}\")\n",
        "\n",
        "    # Read the multiple sequence alignment (MSA) file\n",
        "    records = SeqIO.parse(os.path.join(directory, selected_file), \"fasta\")\n",
        "\n",
        "    # Dictionary to store sequences grouped by family\n",
        "    sequences_by_family = {}\n",
        "\n",
        "    # Process each sequence record\n",
        "    for record in records:\n",
        "        # Check if the record ID contains an underscore\n",
        "        if '_' in record.id:\n",
        "            # Extract family information from the record ID\n",
        "            family = record.id.split(\"_\")[1]\n",
        "\n",
        "            # Add the sequence to the corresponding family in the dictionary\n",
        "            if family in sequences_by_family:\n",
        "                sequences_by_family[family].append(record.seq)\n",
        "            else:\n",
        "                sequences_by_family[family] = [record.seq]\n",
        "        else:\n",
        "            print(f\"Warning: Skipping record with ID '{record.id}' because it does not contain an underscore\")\n",
        "\n",
        "    # Print summary statistics\n",
        "    total_sequences = sum(len(sequences) for sequences in sequences_by_family.values())\n",
        "    print(f\"Total sequences: {total_sequences}\")\n",
        "    \n",
        "    # Uncomment the following code to print the unique families\n",
        "    # print(f\"Total unique families: {len(sequences_by_family)}\")\n",
        "    # for family, sequences in sequences_by_family.items():\n",
        "    #     print(f\"Family {family}: {len(sequences)} sequences\")\n",
        "\n",
        "    # Uncomment the following code to print the sequences grouped by family\n",
        "    # for family, sequences in sequences_by_family.items():\n",
        "    #     print(f\"\\nFamily: {family}\")\n",
        "    #     for sequence in sequences:\n",
        "    #         print(sequence)\n",
        "    \n",
        "    return sequences_by_family\n",
        "\n",
        "sequences = read_fasta()\n",
        "\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pjbEgOe2s3Q",
        "outputId": "03344c1b-ebd1-421e-d9c7-66bbb1bc2222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected File: YDBL_YNBE_20_id90.fas\n",
            "Warning: Skipping record with ID 'null' because it does not contain an underscore\n",
            "Total sequences: 112\n",
            "{'C1DQP2': [Seq('-----------------NEAMGALSGAKARGQLGEMPNGYLGVV--GDQAEEIA...GLF')], 'Q12MD2': [Seq('MKS--FSCLIVGLLLSFQAMALTLQEAKSQGFVGEQLNGYLGLVQTTDEAKALM...SLF')], 'Q8KBR2': [Seq('-----------LAASAASAFALDLETARAKGLVGEVDNGYIAIPPAGTEAQQLV...DIF')], 'F9ZKS4': [Seq('-----------------PVWADSLENLRASGAIGEANTGYV-----EPGAQAEA...DIF')], 'Q5QUL0': [Seq('-MKL-TAVLIAFIGLSSPAQQLNLEDAKEQGLVGEKPDGYLGVVKSTETAEQIV...GLF')], 'F6A9C7': [Seq('LIK--FAGLLLLLSLSLPAHAISLNDAKASGQLGEMPNGYLGVVKSGGQADEIA...GLF')], 'G7EMM2': [Seq('MNTKLNIILVSAVCMSFSAWAISLDDAKNQGLVGEDSSGYLGLVVQNEEAKAVV...ELF')], 'D4F5H1': [Seq('---LGILLALS--LASPA-WALTLDEARASGRVGETLSGYLAARSQDAETLALV...GLF')], 'B6VL59': [Seq('----------------------TLNEAKQQGLVGETFSGYLAVV--DIQAQALV...GLF')], 'I0DZW5': [Seq('MKTIFQALMISGLF-VFSAAALTVDEAKNQGLVGETLSGYLAVVTGNAEVDELV...DLF')], 'G0EA84': [Seq('MRKRLLTATLALMLLSSPALALTLNEARAQGRVGETLNGYLAPLKQDAQTLALV...DLF')], 'Q083T0': [Seq('MKSR-TAVCLTSLLWCTTAWAMTLQEAKTARLVGEQTNGYLGVVIPSPETQQLV...ELF')], 'I4JI09': [Seq('-----------------NEAMSALGAAKAAGLLGEKPDGYLGVV--RGTAEEIA...GLF')], 'A9MQU5': [Seq('MMKILWGLTLS--LLTSSVWALTLDEARTQGRVGETLNGYLVALKNDAETQKLV...DLF')], 'E6WDN1': [Seq('MKRKAIALLLA-TLLIPSAWALTLDQARQQGRVGETLSGYIAARQQDSETLALV...GLF')], 'Q15TY2': [Seq('MKTNLLPTLLAASSLLFASFAIELDDAKAQGLVGEQTDGYLGAVVSKPDVMSLI...ELF')], 'F7NSB9': [Seq('-----------------QQAMSALGPAKTQGLVGEQPNGYLGVV--SAQAKEIA...GLF')], 'A1BGP7': [Seq('-----------VIGLSLPAYALDLETARSQGLAGEVDNGLLAVPPSPGDAQGLI...GLF')], 'Q8X9R5': [Seq('MKRTLLIAVWAIGLMSDSAMALTLNEARSQGRVGETLNGYLVALQTDAETQALV...DLF')], 'D4ZLR5': [Seq('MKT--LLVLAAGLLLSLNAFAISLHDAKSQGLVGEQTNGYLGIVKSSQDANSLA...ELF')], 'A8AGF8': [Seq('MKKQLRATVLILSLLSCNAIALTLSEARTQGRVGETLNGYLVALATDPETRSLV...DLF')], 'D3NUJ7': [Seq('---------------------DALADAKAAGQLGERPDGMVGVVPGPASAQALA...ALF')], 'G6YX77': [Seq('MKRI-MFALVLAFGISASVLAMGLDSVKQQGLVGETPTGYLEVVRAEGQAKEVV...GLF')], 'D3VFF3': [Seq('MKKAG-VILLLNLLFSSFAMSMTLSEAKQQGLVGETFSGYLAPVKNTPDAQSVV...NLF')], 'H2IZB7': [Seq('-----MSMLAVGLLFCSQAFALTLNEAKQQGRVGETLSGYLAPLKQDAETVALA...DLF')], 'D0Z836': [Seq('MKRLLFCAALA--LSLPL-RAVTLDEARASGRVGETLSGYLAARSQDAETLALV...GLF')], 'H3RD19': [Seq('MKRNGITCLLA-LTMSLPAWALTLDEARQSGRVGETLSGYIAPRADDSDTLALV...DLF')], 'B1KPP3': [Seq('MKT--LLVLAAGLLLSLNAFAISLQDAKSQGLVGEQPNGYLGVVKSDPQATAVA...ELF')], 'H2FYQ7': [Seq('-------MFLAAT-LSLSAWALDLQQAKQQGLIGEQLNGLVGAVQGNAQVSAIV...ELF')], 'A3QEY6': [Seq('MKS--ILLLMAGLLLSLNAFAISLQDAKAQGLVGEQINGYLGVVVNNSEASALA...ELF')], 'F8GN59': [Seq('MKMT-RLMAALLLSGSALVMALGLDSAKSQGLVGEQPDGYLGVVKATPDAVELA...TLF')], 'A4XXM4': [Seq('MIR--ISTLLLALSLSLPAYALNLNEAKSSGQLGEQPNGYLGVVKPGGQAEEIA...GLF')], 'G4QJP0': [Seq('MKITIKSVLFGAILFSTVAFAIELDIAKETGLVGEQQNGYLGAVVQSTEVMELI...KLF')], 'C9Y539': [Seq('MRSLVVMLTLL--LASPVSYALTLEEARAQGRVGETLSGYLAPVAQDAETATLV...DLF')], 'A1SSK8': [Seq('MKKI----LMITLMLSFSVFALELSDAKQQGLVGERVDGLLGVVETSLEVTELV...GLF')], 'B0TJK1': [Seq('MKS--LLVLVASLILSFNAFAISLQDAKAQGLVGEQINGYLGVVQNSAEAKAVA...ELF')], 'C0B336': [Seq('YKKYLLSFGLMIALTSVNAIALTLDEARAQGLVGETFSGYIELVQNNKQAQQLV...AIF')], 'Q0C4H8': [Seq('MTTIRTLILGLVLMFGGVAQNSQIETARSTGVIGERIDGYLGVVGSADEIVRQV...DLF')], 'F8GLW5': [Seq('-----------------PAWADALDDLRASGAIGETFEGYV-----EASARAEA...GIF')], 'Q0VNH9': [Seq('MLKPALAMVALLMSMAAF--ALDLGEAKSQGLVGEQTDGYLGVVKATPAAVELA...DLF')], 'Q6LIN9': [Seq('MKN--L-LLFCAIFMSASVFALDLQQAKSQGLVGETNKGYIAVVTASSDVKKLV...DVF')], 'F5RRR4': [Seq('MKRALMLLALG--M-NVHAATLTLNDARAQGRVGETLSGYLAPVQHDAETLALV...DLF')], 'Q1N790': [Seq('-----------------QSPD--LAAALSSGAVGEQADGYMGVAGVSPTVRKEV...ELF')], 'C6XL16': [Seq('---------------------PVLEDVKSAGLVGERYDGYLGLVSAEKAPNDVV...DIF')], 'E3G8V8': [Seq('MKKYLLMVLLSLGLLSQPAMALTLNEARAQGRVGETLSGYLAPLKQDKETLALV...DLF')], 'B6XHW7': [Seq('MMNWFKAIALCSTLSVFSAFALTVEEGKSHGLVGETLSGYLAVIDNSDNAKQLV...DL-')], 'G7UFU6': [Seq('MKRIGLALLLA-TTMSQPAWALTLDEARQSGRVGETLSGYIAPRSDDSETLALV...GLF')], 'A1S5N0': [Seq('MK---WIMMTAAALLSFNVFAMDLQQAKSDGFLGEQQNGYLGLIKDNPEARALM...QLF')], 'B4ETR5': [Seq('YKKYLFSFALITALISSNAVALTLDEARAQGLVGETFSGYVELVQNHKQAQILV...---')], 'A3JD19': [Seq('-----------------DEAKNALDSAKSQGLLGETPSGYLAPV--DARARDIA...GLF')], 'Q3ARR9': [Seq('-----------LGCTTLSAFALDKETARAKGLAGEVDNGLMAIPPASEEAKELI...ALF')], 'H3M4D8': [Seq('MRKRWLIATLALALLSTQALALTLSEARQQGRVGETLNGYLQPLRQDKETLALV...DLF')], 'G0AXI5': [Seq('MKSR-LLLLLTLSVLSLSAFAMSLQDAKSQGYLGEQANGYLGLVQANPEAKAVM...KLF')], 'A6T8G4': [Seq('MRKRLLTAALTLALLSAPALALTLSEARQQGRVGETLNGYLAPLRQDKETLALV...DLF')], 'D3V1F2': [Seq('MKKAG-VIFLASLLFSSFAVGMTLSEAKQQGLVGETFSGYLAPVKKTRDTQSVI...NLF')], 'D2TAR3': [Seq('MRQRYRGLLLGCLLLSPAAIALTLDEARQQGRVGETLNGYIAPIQQDGETLQLV...NLF')], 'D4E628': [Seq('MKKRVLWLMAIGGLFSSMAMALTLDEAKTQGRLGETLSGYVAPVKQDAQTLELV...DLF')], 'Q47V16': [Seq('MNKIIVSLVLIASSIAFSAWAVTLDQAKQQGLVGEMSNGYLGVVVASTEVSDLV...DIF')], 'E7W9V0': [Seq('MKKYLMLWVLTLSLLTPSVWALTLDEARTQGRVGETLNGYLVALKNDAETQKLV...DLF')], 'Q7N512': [Seq('MKRTGTILLLASVLFSSSVVGMTLNEAKQKGLVGETFSGYLAIVKTDNQAQALV...GLF')], 'A4SED8': [Seq('-----------VVAMTASASALDLDAARAKGLAGEVDNGLLAVPPASNEAAPVI...GLF')], 'I1E1Q2': [Seq('-----------------QQAMAALGNAKARGLVGEQANGYLGVV--SNDAADIV...GLF')], 'F2PEW3': [Seq('MFSK-FHALIISVLCSANAMALTLQQAKQQGLIGEANTGYVASIQPTAQVKQLV...V-F')], 'B4S846': [Seq('-----------ILAISSAAFALDLETARSQGLAGETDSGLLAKPPADNAAVSLI...GLF')], 'G9Y1Q6': [Seq('MRKSLIALLGAGLLFSSAVFALTLDQAKQQGRVGETLNGYIVALKQDKETLALV...GLF')], 'I3TI74': [Seq('AARFLAVLLAAALPAAAVAREAGLDALKAQGLVGERFDGYVGAVKSQTATQQVI...ELF')], 'B3EJM8': [Seq('-----------LLSFTTSAFAIDLKTALSKGLAGEVDSGYLAIPPASGEARPLV...DIF')], 'H5V5C9': [Seq('MKR-NAALLAG-LLLCQPVMALTLDEARQQGRVGETLDGYIAPRAQDSETLALV...DIF')], 'Q2BNA3': [Seq('NLKAILTSIILAISLVAAAK----DDAKAQGLIGERNNGYLGIITSSPDLKNLV...DIF')], 'G7EY71': [Seq('LLKKLNVILLSAALLSFSAWAISLDDAKNQGLVGEDSSGYLGLVVQNAEAKAVV...ELF')], 'A8FWT4': [Seq('MKT--LLVLAAGLLLSLNAFAISLQDAKSQGLVGEQPNGYLGVVKSNPESNSVA...ELF')], 'D1NZ33': [Seq('MINLIKVIILFSAFSIFSAFALTVDEGKNQGLIGETFSGYLAVVDNNPEAKVLI...DL-')], 'D4I375': [Seq('MKHLYRVLLLGCLLLPPAAMALTLNEARQQGRVGETLNGYIAPIQQDAETRELV...GLF')], 'C4U8L8': [Seq('-------MALASLLFSSMAFALTLEQAKQQGRVGETLNGYLAAVKKDAETLALV...NLF')], 'Q0HUM9': [Seq('MKSP-LLLLLGLSLFSLNAFAMSLQEAKSQGYLGEQLNGYLGLVQNNSEAKAVM...KLF')], 'C1M4X6': [Seq('MRKAVALVVLS--LLSSNTWALTLNEARTQGRVGETLNGYLVALQTDAETQALV...DLF')], 'E8RK18': [Seq('MKRWIMTVLFL--LLCGQALALDLQTAKAQGLVGETPSGYLAQVQGNGETAQLV...GLF')], 'D4Z3B8': [Seq('-----------------QSGA--VAAAMSAGTVGEQADGYLGIAGVSDAVRQEV...ELF')], 'B3EC79': [Seq('-----------FAGFAAPAFALDLETARSQGIAGEMDNGLLAVPPAGSDAQGLI...GLF')], 'F8VIK8': [Seq('MKKILWALMLS--LLTPSVWALTLEEARTQGRVGETLNGYLAALTPDAETQKLV...DLF')], 'H3ZJD6': [Seq('-----------------QQAMAALPNAKAQGLVGEQPNGYLGVV--GTDTELLV...GLF')], 'G0BHD3': [Seq('MKKRYLGLVGVAWLFSSMAMALTLDEAKQQGRVGETLSGYIAPVKQDAETLALV...GLF')], 'B4X4Z4': [Seq('MKMK-SALAMVALLLSMAAFALDLDAAKSQGLVGEQTDGYLGVVKATPAAVELA...DLF')], 'F2N0W1': [Seq('MKAL-LTSLLLALLLALPAAAMTLNEAKASGMLGEKPDGYLGVVRSSKNAEEIA...GLF')], 'E2ZSB3': [Seq('-----------------GQAMSALGGAKAQGLVGEQADGYLGVV--SGQAADIA...GLF')], 'Q3IHX3': [Seq('MKVNLSIILVSAVCMSFSAWAISLDDAKSQGLVGEDSSGYLGLVVQNAEAKVVV...ELF')], 'G7FIV2': [Seq('-------------------WAMSLDEAKSQGLVGENSSGYLGLV--NTEAKAVV...ELF')], 'B9Z1T8': [Seq('MKTTIRGMALLLLVCSTLAMALDLDGAKSQGLVGEQPNGYLGVVKATPQAVELA...ALF')], 'B8CR35': [Seq('MKS--ILVLFASLLLSFNALAISLQDAKAQGLVGEQTNGYLGLVQASAEAKTLI...ELF')], 'D2ZBZ0': [Seq('MKRLLILLALG--M-NVQAATLTLNEARTQGRVGETLSGYLAPVKQDPDTLALV...DLF')], 'F4AQT8': [Seq('MKKSLLTTFVAATSLFFTSFAVELDDAKDQGLVGEQTDGYLGAVVSQPDVIALI...ELF')], 'Q0YQF8': [Seq('-----------ALSITLPAYALDLDTARSSGLAGEVDNGLLALPPASTAAQELI...GLF')], 'B4S9U0': [Seq('-----------VMGMALTAFALDLDTARSSGLAGEVDNGLLAIPPAASDARTLI...GLF')], 'F1Z870': [Seq('-----------------QARDPLYSAARAKGEVGEQVNGYLGFVTPAPELRKVI...EIF')], 'B1EN20': [Seq('MKKILLVVVWIISLMSSNVMALTLDEARAQGRVGETLNGYLVALKTDAETQTLV...DLF')], 'G4KDG9': [Seq('MKKLTFGVILSSLVFSSGAFALTLEQAKQQGRVGETLSGYLAPVKKDAETLALV...NLF')], 'C7R7B6': [Seq('LLRKSLLTLLATFIFAAQ--ALDIGTLKDQGVVGEMSTGYVGIVVSNPQIRSFV...G-F')], 'G2II45': [Seq('-----------------QDSV--VSAAKAAGTVGEQADGYLGTKGVSAEVRAAV...ELF')], 'A4WA91': [Seq('KQRAIGLLVLA--I-SAQVQAITLNEARAQGRVGETLSGYLAPVRQDPETLALV...DLF')], 'Q1CIR6': [Seq('MKKHTLGLVLTSLLYSPHSFALTLEQAKQQGRVGETLSGYLAPVKKDAETLALV...NLF')], 'H8DJL4': [Seq('MRRKSAALLLA-LMMMQPAWALTLDQARQSGLVGETLSGYLAARAGDSETQALV...GLF')], 'A6EV80': [Seq('-----------------QEAKSQLDTAKQQGLVGETPTGYLDVV--SGNAQGIV...GLF')], 'G9Z7B2': [Seq('MKKSLIGLFLGLCVLSSGAWALSLNEARQQGRVGETLSGYVGAIKQDKETLGLV...DLF')], 'Q3B3V0': [Seq('-----------LMALAAPASALDLDTARSSGLAGEVDTGLLAIPPAAQDAATLI...GLF')], 'C4UFK3': [Seq('MKKQSIGSLILSLLFSASGWALSLDQAKQQGRVGETLSGYLAPIKKDTETLALV...DL-')], 'I2B956': [Seq('MNKIYQFLRLASLLWSTAAAALTLDEARQQGRAGETLSGYLAPRASDEQTLALV...DLF')], 'B3QPI1': [Seq('-----------LFAFTSSAFALDLETALTKGLVGEVDDGYIAIPPAGNEAAELV...DIF')], 'Q1NX14': [Seq('-----------------SQAMDALGDAKARGLVGEQADGYLGVV--EGQAATIA...GLF')], 'I0QV63': [Seq('-----MKLMLLGSLWCNAALALTVEQAKQQGLVGETLSGYLAPVKQGAQTQALV...DLF')], 'A8H3N3': [Seq('MKS--LLVLFAALLLSFNAFAISLQDAKAQGLVGEQLNGYLGVVKSSAEAKSVV...ELF')], 'D4BU54': [Seq('MNRIIKVFALCSLLTVFQAIAITVDEAKETGLLGETLSGYLAPINSNPEATKLA...DL-')], 'Q0EW75': [Seq('MCYNVMPLFALMLGLSSMVWAMDLHDAKASGLVGEKPDGYLGVVQGGAAVAALV...GLF')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def read_sequence(sequences):\n",
        "    # Flatten the list of sequences\n",
        "    all_sequences = [seq for seq_list in sequences.values() for seq in seq_list]\n",
        "    # Select a random sequence from the list\n",
        "    return random.choice(all_sequences)\n"
      ],
      "metadata": {
        "id": "yNwGEgXm270-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = read_sequence(sequences)\n",
        "print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLvZUcM73f-Z",
        "outputId": "d6417d73-44e7-46aa-ec3d-bf04791b4650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------TLNEAKQQGLVGETFSGYLAVV--DIQAQALVIKINQAREKKYAEVATSNNISTEQVAKLAGEKLVNRAEPGEYVQG-NGQWLKK---------MSSLILSGCV-RLEVATPEKPININMNVKIEHEIHVKIDKQIEEMLKSNTGLF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import amin\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encoding(seq):\n",
        "    # Define the amino acid alphabet\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "    # Create a dictionary to map each amino acid to its index in the alphabet\n",
        "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot = np.zeros([len(seq), len(amino_acids)])\n",
        "    # Set the appropriate elements to 1\n",
        "    for i, aa in enumerate(seq):\n",
        "      if aa in aa_to_index:\n",
        "        one_hot[i, aa_to_index[aa]] = 1\n",
        "    return one_hot\n"
      ],
      "metadata": {
        "id": "7JT9bogS3gRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot = one_hot_encoding(seq)\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qbMStla33cs",
        "outputId": "e6905bc5-a41e-4708-d919-7ea49ed85416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto encoder \n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Define the amino acid alphabet\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "\n",
        "def one_hot_encoding(seq):\n",
        "    # Create a dictionary to map each amino acid to its index in the alphabet\n",
        "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot = np.zeros((len(seq), len(amino_acids)))\n",
        "    # Set the appropriate elements to 1\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_index:\n",
        "            one_hot[i, aa_to_index[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Define the size of the latent space\n",
        "latent_dim = 32\n",
        "\n",
        "# Define the architecture of the autoencoder\n",
        "input_seq = Input(shape=(None, len(amino_acids)))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_seq)\n",
        "decoded = Dense(len(amino_acids), activation='softmax')(encoded)\n",
        "autoencoder = Model(input_seq, decoded)\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Encode the sequence using one-hot encoding\n",
        "seq_encoded = one_hot_encoding(seq).reshape(1, -1, len(amino_acids))"
      ],
      "metadata": {
        "id": "w5p9UGyg33gO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder on the encoded sequence\n",
        "autoencoder.fit(seq_encoded, seq_encoded, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWZhT6g44RUT",
        "outputId": "18daa990-5dde-4af0-85a6-e9c5e9305467"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 739ms/step - loss: 3.0023\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9952\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9880\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9809\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9738\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9667\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.9596\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9525\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9455\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9384\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9313\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9242\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9171\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9100\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9029\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8957\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8886\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8815\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8744\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8673\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8602\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8530\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8459\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8388\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8316\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8244\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8172\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8100\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8027\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7954\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7882\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7809\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7736\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7663\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7589\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7516\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7441\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7367\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7291\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7216\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7140\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7063\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6986\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6909\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6831\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6752\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6673\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6593\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6513\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6432\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6351\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6268\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6185\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6102\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6017\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5933\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5848\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.5762\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5676\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5589\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5501\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5413\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5325\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5236\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5146\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5057\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4966\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4875\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4783\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4691\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4598\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4505\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4411\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4316\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4220\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4124\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.4028\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3930\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3831\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3732\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3633\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3533\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3432\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3331\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3229\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3126\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3023\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2919\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2814\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2709\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2603\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2497\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2390\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2283\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2175\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2067\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1958\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1849\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1739\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20d9657580>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder to encode and decode the sequence\n",
        "seq_decoded = autoencoder.predict(seq_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqNhNzaG4RX2",
        "outputId": "93f0f701-94c7-4f36-84a2-9e36baae0f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f75c627c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 196ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute reconstruction error metrics\n",
        "cross_entropy = -np.mean(np.sum(seq_encoded * np.log(seq_decoded), axis=-1))\n",
        "same_aa_accuracy = np.mean(np.argmax(seq_encoded, axis=-1) == np.argmax(seq_decoded, axis=-1))\n",
        "most_common_aa_accuracy = np.mean(np.argmax(seq_encoded.sum(axis=0), axis=-1) == np.argmax(seq_decoded.sum(axis=0), axis=-1))"
      ],
      "metadata": {
        "id": "IIpsMeUq32VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-entropy:\", cross_entropy)\n",
        "print(\"Same amino acid accuracy:\", same_aa_accuracy)\n",
        "print(\"Most common amino acid accuracy:\", most_common_aa_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAArhWLM42kD",
        "outputId": "b06c3219-e25e-4109-a36f-95470b9163c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-entropy: 2.1078780508605686\n",
            "Same amino acid accuracy: 0.8994082840236687\n",
            "Most common amino acid accuracy: 0.8994082840236687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8N0GLY4wFFYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encoding(seq):\n",
        "    # Define the amino acid alphabet\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "    # Create a dictionary to map each amino acid to its index in the alphabet\n",
        "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot = np.zeros((len(seq), len(amino_acids)))\n",
        "    # Set the appropriate elements to 1\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_index:\n",
        "            one_hot[i, aa_to_index[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "def decode_sequence(one_hot_seq):\n",
        "    # Define the amino acid alphabet\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "    # Convert the one-hot encoded sequence back into an amino acid sequence\n",
        "    return ''.join([amino_acids[i] for i in np.argmax(one_hot_seq, axis=-1)])\n",
        "\n",
        "# Define the size of the latent space\n",
        "latent_dim = 32\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_seq = Input(shape=(None, len(amino_acids)))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_seq)\n",
        "decoded = Dense(len(amino_acids), activation='softmax')(encoded)\n",
        "autoencoder = Model(input_seq, decoded)\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "\n",
        "# Encode the sequence using one-hot encoding\n",
        "seq_encoded = one_hot_encoding(seq).reshape(1, -1, len(amino_acids))\n",
        "\n",
        "# Train the autoencoder on the encoded sequence\n",
        "autoencoder.fit(seq_encoded, seq_encoded, epochs=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqKlLAz42mh",
        "outputId": "31d7ce78-1d65-49ee-acba-15c0cd984765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.0437\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.0363\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.0290\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0216\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0143\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.0070\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9997\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9924\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9852\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9779\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9707\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9635\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9563\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9491\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.9420\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.9349\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9277\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9206\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.9136\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.9066\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.8996\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8926\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8855\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8785\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8714\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.8643\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.8571\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8500\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.8428\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8356\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8284\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8212\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8139\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8066\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.7992\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7918\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.7845\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7770\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7696\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.7621\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.7545\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7469\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7393\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7317\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7240\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7162\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7085\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.7008\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.6931\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.6853\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6774\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6695\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6616\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6536\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6455\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6374\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6293\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6211\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6128\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6045\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5962\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.5877\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5792\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5707\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5621\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5535\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5448\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5360\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5272\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5184\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5095\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5005\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4915\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4824\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4732\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4639\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4546\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4451\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4356\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4261\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4165\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4069\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3971\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3874\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3775\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3676\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3577\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.3477\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3377\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3276\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3175\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3073\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2970\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2867\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2763\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2659\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2554\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2448\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2342\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2235\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f75c61a2fe0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder to encode and decode the sequence\n",
        "seq_decoded = autoencoder.predict(seq_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "945Ir2lZ42op",
        "outputId": "4edd7af3-2772-4a03-f7a2-7d7eb2c4ecc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f75c627fe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 301ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute reconstruction error metrics\n",
        "cross_entropy = -np.mean(np.sum(seq_encoded * np.log(seq_decoded), axis=-1))\n",
        "same_aa_accuracy = np.mean(np.argmax(seq_encoded, axis=-1) == np.argmax(seq_decoded, axis=-1))\n",
        "most_common_aa_accuracy = np.mean(np.argmax(seq_encoded.sum(axis=0), axis=-1) == np.argmax(seq_decoded.sum(axis=0), axis=-1))\n",
        "\n",
        "print(\"Cross-Entropy:\", cross_entropy)\n",
        "print(\"Accuracy (Same aa):\", same_aa_accuracy)\n",
        "print(\"Accuracy (Mode aa):\", most_common_aa_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9F3qxYT42rV",
        "outputId": "1a3f7106-43ad-43e0-ea62-c8fa44f8447f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy: 2.2128596658537374\n",
            "Accuracy (Same aa): 0.8224852071005917\n",
            "Accuracy (Mode aa): 0.8224852071005917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print the original and reconstructed sequences\n",
        "print(\"Original sequence:\")\n",
        "print(decode_sequence(seq_encoded[0]))\n",
        "print(\"Reconstructed sequence:\")\n",
        "print(decode_sequence(seq_decoded[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulRs2c4Y42t9",
        "outputId": "4c9312d8-d2b8-4311-8b7b-af8ef2f8d8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sequence:\n",
            "----------------------TLNEAKQQGLVGETFSGYLAVV--DIQAQALVIKINQAREKKYAEVATSNNISTEQVAKLAGEKLVNRAEPGEYVQG-NGQWLKK---------MSSLILSGCV-RLEVATPEKPININMNVKIEHEIHVKIDKQIEEMLKSNTGLF\n",
            "Reconstructed sequence:\n",
            "----------------------VLNEAKQQGLVGEVGIGGLAVV--EIQAQALVIKINQAEEKKGAEVAVINNIIVEQVAKLAGEKLVNEAEIGEGVQG-NGQVLKK---------MIILILIG-V-ELEVAVIEKIININMNVKIEKEIKVKIEKQIEEMLKINVGLG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE HAVE DONE WITH THE AMION ACIDS SEQUNCES AS = 'ACDEFGHIKLMNPQRSTVWY-'"
      ],
      "metadata": {
        "id": "mUrIEajSzQdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mY3wKhGOfEeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to do encoding and decoing for the missing amion acids and we need to train the data "
      ],
      "metadata": {
        "id": "n8yOF2HYe6Ua"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now i am taking the amni sequences as 'S, A, G, N, G, R, Y, E, I, G, A,N'"
      ],
      "metadata": {
        "id": "nIGpQT52fWjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Encoder \n",
        "\n",
        "# Define the amino acid alphabet\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "\n",
        "def one_hot_encoding(seq):\n",
        "    # Create a dictionary to map each amino acid to its index in the alphabet\n",
        "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot = np.zeros((len(seq), len(amino_acids)))\n",
        "    # Set the appropriate elements to 1\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_index:\n",
        "            one_hot[i, aa_to_index[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Define the size of the latent space\n",
        "latent_dim = 32\n",
        "\n",
        "# Define the architecture of the autoencoder\n",
        "input_seq = Input(shape=(None, len(amino_acids)))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_seq)\n",
        "decoded = Dense(len(amino_acids), activation='softmax')(encoded)\n",
        "autoencoder = Model(input_seq, decoded)\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Encode the sequence using one-hot encoding\n",
        "seq_encoded = one_hot_encoding(seq).reshape(1, -1, len(amino_acids))\n",
        "\n",
        "# Train the autoencoder on the encoded sequence\n",
        "autoencoder.fit(seq_encoded, seq_encoded, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lCTp0EBfHRR",
        "outputId": "0a1262ca-25b4-4506-cee8-9cde89095066"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 2.9850\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.9780\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9710\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.9640\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.9570\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9500\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9429\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9359\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9289\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9218\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9148\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.9077\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9007\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8936\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8866\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8795\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8724\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8653\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8582\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8511\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8439\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8368\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8296\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8224\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8152\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8079\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8006\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7933\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7859\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7785\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7711\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7637\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7562\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7487\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7412\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7336\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.7260\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7183\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7106\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7028\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6950\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6872\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6793\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6714\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6634\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6553\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6472\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6390\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6309\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6226\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6143\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6059\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5975\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5890\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5805\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5719\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5632\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5545\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5456\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5367\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5278\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5187\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5096\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5005\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4912\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4819\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4726\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4632\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4537\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4442\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4346\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4249\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4152\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4054\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3955\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3856\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3755\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3654\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3552\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3450\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3346\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3243\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3138\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3033\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2927\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2821\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2714\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2607\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2499\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2390\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2282\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2172\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2062\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1952\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1841\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1730\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1618\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1506\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1393\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1279\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20daed8fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder to encode and decode the sequence\n",
        "seq_decoded = autoencoder.predict(seq_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chKWHTAxfrCT",
        "outputId": "e5048c2f-37cf-4fbc-c2de-d8079463d0ed"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 111ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute reconstruction error metrics\n",
        "cross_entropy = -np.mean(np.sum(seq_encoded * np.log(seq_decoded), axis=-1))\n",
        "same_aa_accuracy = np.mean(np.argmax(seq_encoded, axis=-1) == np.argmax(seq_decoded, axis=-1))\n",
        "most_common_aa_accuracy = np.mean(np.argmax(seq_encoded.sum(axis=0), axis=-1) == np.argmax(seq_decoded.sum(axis=0), axis=-1))"
      ],
      "metadata": {
        "id": "uD_FvTQ9fvJC"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-entropy:\", cross_entropy)\n",
        "print(\"Same amino acid accuracy:\", same_aa_accuracy)\n",
        "print(\"Most common amino acid accuracy:\", most_common_aa_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEzZKgpVfvUv",
        "outputId": "930ec1fa-37ca-4f84-be61-d7e5b9afb53f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-entropy: 2.2881636252770057\n",
            "Same amino acid accuracy: 0.863905325443787\n",
            "Most common amino acid accuracy: 0.863905325443787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print the original and reconstructed sequences\n",
        "print(\"Original sequence:\")\n",
        "print(decode_sequence(seq_encoded[0]))\n",
        "print(\"Reconstructed sequence:\")\n",
        "print(decode_sequence(seq_decoded[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqof3Yw8f5ba",
        "outputId": "7f92f4ae-8304-41d4-ce52-1a53d7074109"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sequence:\n",
            "MMKILWGLTLS--LLTSSVWALTLDEARTQGRVGETLNGYLVALKNDAETQKLVLDINRARRASYQQLADSNHLPVDEVAKMAGQKLVERARPGEYVQGINGKWLRK-MKMIIVMLSVFFMVVSCTPRIEVAAPEQPITINMNVKIEHEIHIKVDKDVEELLKSRSDLF\n",
            "Reconstructed sequence:\n",
            "MMKILWGLTLD--LLTDDVWALTLDEARTQGRVGETLKGLLVALKKDAETQKLVLDIKRARRADLQQLADDKHLPVDEVAKMAGQKLVERARPGELVQGIKGKWLRK-MKMIIVMLDVGGMVVDVTPRIEVAAPEQPITIKMKVKIEHEIHIKVDKDVEELLKDRDDLG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding with the missing amino acids \n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encoding(seq):\n",
        "    # Define the amino acid alphabet\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "    # Create a dictionary to map each amino acid to its index in the alphabet\n",
        "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot = np.zeros((len(seq), len(amino_acids)))\n",
        "    # Set the appropriate elements to 1\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_index:\n",
        "            one_hot[i, aa_to_index[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "def decode_sequence(one_hot_seq):\n",
        "    # Define the amino acid alphabet\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "    # Convert the one-hot encoded sequence back into an amino acid sequence\n",
        "    return ''.join([amino_acids[i] for i in np.argmax(one_hot_seq, axis=-1)])\n",
        "\n",
        "# Define the size of the latent space\n",
        "latent_dim = 32\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_seq = Input(shape=(None, len(amino_acids)))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_seq)\n",
        "decoded = Dense(len(amino_acids), activation='softmax')(encoded)\n",
        "autoencoder = Model(input_seq, decoded)\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "\n",
        "# Encode the sequence using one-hot encoding\n",
        "seq_encoded = one_hot_encoding(seq).reshape(1, -1, len(amino_acids))\n",
        "\n",
        "# Train the autoencoder on the encoded sequence\n",
        "autoencoder.fit(seq_encoded, seq_encoded, epochs=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2WckDhaf4r4",
        "outputId": "f8c37edc-ec74-40c5-bb27-a68d04a1ad76"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 736ms/step - loss: 3.0488\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0425\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0361\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.0298\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0234\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0171\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0108\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0044\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.9981\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9917\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9853\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.9789\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9725\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9662\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9598\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9534\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.9469\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9405\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9341\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9276\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9212\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.9147\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9082\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9017\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8952\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8886\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8821\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8755\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8689\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.8623\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8557\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8490\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8424\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8358\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8291\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8224\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8157\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8089\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8022\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7954\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7886\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7818\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7749\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7680\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7610\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7540\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7470\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7399\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7329\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7258\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7186\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7114\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7041\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6968\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6895\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6821\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6746\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6671\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6596\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6520\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6444\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6367\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6289\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6211\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6132\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6053\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5973\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5892\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5811\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5729\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5647\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5564\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5481\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5397\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5313\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5227\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5142\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5056\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4969\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4881\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4792\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4703\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4612\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4522\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4430\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4339\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4246\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4153\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4059\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3964\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3869\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3773\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3677\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3580\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3482\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3384\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3285\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3185\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3085\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2984\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20d9a4dea0>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder to encode and decode the sequence\n",
        "seq_decoded = autoencoder.predict(seq_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1MTWRXff4-N",
        "outputId": "7d6849e6-4308-4eb0-c2d8-d73ad5b30190"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-entropy:\", cross_entropy)\n",
        "print(\"Same amino acid accuracy:\", same_aa_accuracy)\n",
        "print(\"Most common amino acid accuracy:\", most_common_aa_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ1lSvG2hAuF",
        "outputId": "38ea4750-6e4c-4d65-bed5-b08fe21cab96"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-entropy: 2.2881636252770057\n",
            "Same amino acid accuracy: 0.863905325443787\n",
            "Most common amino acid accuracy: 0.863905325443787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print the original and reconstructed sequences\n",
        "print(\"Original sequence:\")\n",
        "print(decode_sequence(seq_encoded[0]))\n",
        "print(\"Reconstructed sequence:\")\n",
        "print(decode_sequence(seq_decoded[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiVDMAVUf5fc",
        "outputId": "efe0a1ef-4ea1-4e82-fcb3-dddcdeb2c2b4"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sequence:\n",
            "MMKILWGLTLS--LLTSSVWALTLDEARTQGRVGETLNGYLVALKNDAETQKLVLDINRARRASYQQLADSNHLPVDEVAKMAGQKLVERARPGEYVQGINGKWLRK-MKMIIVMLSVFFMVVSCTPRIEVAAPEQPITINMNVKIEHEIHIKVDKDVEELLKSRSDLF\n",
            "Reconstructed sequence:\n",
            "MMKILWGLTLD--LLTDDVWALTLDEARTQGRVGETLKGLLVALKKDAETQKLVLDIKRARRADLQQLADDKHLPVDEVAKMAGQKLVERARPGELVQGIKGKWLRK-MKMIIVMLDVGGMVVDVTPRIEVAAPEQPITIKMKVKIEHEIHIKVDKDVEELLKDRDDLG\n"
          ]
        }
      ]
    }
  ]
}